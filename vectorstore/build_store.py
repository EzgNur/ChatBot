"""
Vector store oluÅŸturma modÃ¼lÃ¼ - Temiz JSON veriler iÃ§in optimize edilmiÅŸ
FAISS kullanarak embedding'leri saklama ve arama yapma
"""

import os
import json
from typing import List, Dict
from datetime import datetime
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

class OptimizedVectorStoreBuilder:
    def __init__(self, embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Optimize edilmiÅŸ vector store builder sÄ±nÄ±fÄ±
        Temiz JSON veriler iÃ§in tasarlandÄ±
        """
        self.embedding_model = embedding_model
        print(f"ğŸ¤– Embedding modeli yÃ¼kleniyor: {embedding_model}")
        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
        
        # Optimize edilmiÅŸ chunking parametreleri
        # Ä°stek: chunk_size 400â€“500, overlap 120â€“150; sayÄ±larÄ±/baÅŸlÄ±klarÄ± kÄ±rmayÄ± azalt
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=450,
            chunk_overlap=120,
            length_function=len,
            separators=[
                "\n\n# ",  # baÅŸlÄ±k sÄ±nÄ±rlarÄ±
                "\n\n## ",
                "\n\n",
                "\n",
                ". ",
                ": ",  # madde/baÅŸlÄ±k baÄŸlarÄ±
                ", ",
                " ",
                ""
            ]
        )
        print("âœ… Text splitter hazÄ±rlandÄ±")
    
    def load_clean_json_data(self, filepath: str) -> List[Document]:
        """
        Temiz JSON verisini yÃ¼kler ve Document objelerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
        """
        print(f"ğŸ“‚ JSON verisi yÃ¼kleniyor: {filepath}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"âœ… {len(data)} blog yazÄ±sÄ± yÃ¼klendi")
        
        documents = []
        total_words = 0
        
        for item in data:
            if item.get('content') and len(item['content'].strip()) > 50:  # Ã‡ok kÄ±sa iÃ§erikleri filtrele
                # BaÅŸlÄ±k + Ä°Ã§erik birleÅŸtir (daha iyi context iÃ§in)
                full_content = f"BaÅŸlÄ±k: {item.get('title', '')}\n\n{item['content']}"
                
                doc = Document(
                    page_content=full_content,
                    metadata={
                        "title": item.get('title', ''),
                        "author": item.get('author', 'Oktay Ã–zdemir'),
                        "date": item.get('date', ''),
                        "url": item.get('url', ''),
                        "word_count": item.get('word_count', 0),
                        "content_type": item.get('content_type', 'blog_post'),
                        "language": item.get('language', 'tr'),
                        "domain": item.get('domain', 'law_immigration_politics'),
                        "source_id": item.get('id', len(documents) + 1)
                    }
                )
                documents.append(doc)
                total_words += item.get('word_count', 0)
        
        print(f"ğŸ“ Toplam {total_words:,} kelime iÅŸlenecek")
        return documents

    def add_texts_with_metadata(self, texts: List[str], metadatas: List[Dict], save_path: str = None):
        """
        Var olan FAISS'e metinleri ekler; yoksa yeni bir FAISS oluÅŸturur.
        """
        if not save_path:
            save_path = os.path.join(os.path.dirname(__file__), "..", "data", "vectorstore")
        os.makedirs(save_path, exist_ok=True)

        try:
            vectorstore = FAISS.load_local(save_path, self.embeddings, allow_dangerous_deserialization=True)
        except Exception:
            vectorstore = None

        if vectorstore is None:
            documents = [Document(page_content=t, metadata=m) for t, m in zip(texts, metadatas)]
            vectorstore = FAISS.from_documents(documents, self.embeddings)
        else:
            vectorstore.add_texts(texts, metadatas=metadatas)

        vectorstore.save_local(save_path)
        return True

    def add_transcript_to_vectorstore(self, text: str, meta: Dict | None = None, save_path: str = None) -> int:
        """
        Video transkript metnini mevcut vectorstore'a ekler.
        DÃ¶nÃ¼ÅŸ: eklenen chunk sayÄ±sÄ±
        """
        if not text or not text.strip():
            return 0
        meta = meta or {}
        chunks = self.text_splitter.split_text(text)
        metadatas: List[Dict] = []
        for _ in chunks:
            metadatas.append({
                "title": meta.get("title", "Video Transcript"),
                "url": meta.get("url", ""),
                "author": meta.get("author", "Video"),
                "date": meta.get("date", datetime.now().strftime("%d/%m/%Y")),
                "source_type": meta.get("source_type", "video_transcript"),
                "video_id": meta.get("video_id"),
                "duration": meta.get("duration"),
            })

        self.add_texts_with_metadata(chunks, metadatas, save_path=save_path)
        return len(chunks)
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        DokÃ¼manlarÄ± kÃ¼Ã§Ã¼k parÃ§alara bÃ¶ler
        """
        split_docs = self.text_splitter.split_documents(documents)
        print(f"{len(documents)} dokÃ¼man {len(split_docs)} parÃ§aya bÃ¶lÃ¼ndÃ¼")
        return split_docs
    
    def build_vectorstore(self, documents: List[Document], save_path: str = None) -> FAISS:
        """
        FAISS vector store oluÅŸturur
        """
        if not save_path:
            save_path = os.path.join(os.path.dirname(__file__), "..", "data", "vectorstore")
        
        os.makedirs(save_path, exist_ok=True)
        
        print("Embedding'ler oluÅŸturuluyor...")
        vectorstore = FAISS.from_documents(documents, self.embeddings)
        
        # Vector store'u kaydet
        vectorstore.save_local(save_path)
        print(f"Vector store kaydedildi: {save_path}")
        
        return vectorstore
    
    def load_vectorstore(self, load_path: str = None) -> FAISS:
        """
        KaydedilmiÅŸ vector store'u yÃ¼kler
        """
        if not load_path:
            load_path = os.path.join(os.path.dirname(__file__), "..", "data", "vectorstore")
        
        try:
            vectorstore = FAISS.load_local(load_path, self.embeddings, allow_dangerous_deserialization=True)
            print(f"Vector store yÃ¼klendi: {load_path}")
            return vectorstore
        except Exception as e:
            print(f"Vector store yÃ¼klenirken hata: {e}")
            return None
    
    def process_clean_json_to_vectorstore(self, clean_json_filepath: str):
        """
        Temiz JSON veriden vector store'a kadar tÃ¼m iÅŸlemi yapar
        """
        print("ğŸš€ Vector Store oluÅŸturma sÃ¼reci baÅŸlÄ±yor...\n")
        
        # 1. Temiz JSON verisini yÃ¼kle
        print("1ï¸âƒ£ JSON verisi yÃ¼kleniyor...")
        documents = self.load_clean_json_data(clean_json_filepath)
        
        if not documents:
            print("âŒ HiÃ§ dokÃ¼man yÃ¼klenemedi!")
            return None
        
        # 2. DokÃ¼manlarÄ± chunk'lara bÃ¶l
        print("\n2ï¸âƒ£ DokÃ¼manlar chunk'lara bÃ¶lÃ¼nÃ¼yor...")
        split_docs = self.split_documents(documents)
        
        # 3. Ä°ÅŸlenmiÅŸ verileri kaydet
        print("\n3ï¸âƒ£ Ä°ÅŸlenmiÅŸ veriler kaydediliyor...")
        processed_dir = os.path.join(os.path.dirname(__file__), "..", "data", "processed")
        os.makedirs(processed_dir, exist_ok=True)
        
        processed_data = []
        for i, doc in enumerate(split_docs):
            processed_data.append({
                "chunk_id": i + 1,
                "content": doc.page_content,
                "metadata": doc.metadata,
                "chunk_length": len(doc.page_content),
                "processed_at": datetime.now().isoformat()
            })
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        processed_filepath = os.path.join(processed_dir, f"processed_chunks_{timestamp}.json")
        with open(processed_filepath, 'w', encoding='utf-8') as f:
            json.dump(processed_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Ä°ÅŸlenmiÅŸ veriler kaydedildi: {processed_filepath}")
        
        # 4. Vector store oluÅŸtur
        print("\n4ï¸âƒ£ Vector store oluÅŸturuluyor...")
        vectorstore = self.build_vectorstore(split_docs)
        
        # 5. Ã–zet rapor
        print(f"\nğŸ¯ Ä°ÅLEM Ã–ZETÄ°:")
        print(f"   ğŸ“„ Toplam dokÃ¼man: {len(documents)}")
        print(f"   ğŸ§© Toplam chunk: {len(split_docs)}")
        print(f"   ğŸ“Š Ortalama chunk boyutu: {sum(len(doc.page_content) for doc in split_docs) // len(split_docs)} karakter")
        print(f"   ğŸ¤– Embedding modeli: {self.embedding_model}")
        
        return vectorstore

def test_chunking():
    """
    Chunking iÅŸlemini test et
    """
    print("ğŸ§ª Chunking testi baÅŸlÄ±yor...\n")
    
    builder = OptimizedVectorStoreBuilder()
    
    # En son temiz JSON dosyasÄ±nÄ± bul
    raw_data_dir = os.path.join(os.path.dirname(__file__), "..", "data", "raw")
    
    if not os.path.exists(raw_data_dir):
        print("âŒ Ham veri klasÃ¶rÃ¼ bulunamadÄ±!")
        return
    
    json_files = [f for f in os.listdir(raw_data_dir) if f.startswith('clean_blog_data_') and f.endswith('.json')]
    if not json_files:
        print("âŒ Temiz blog verisi bulunamadÄ±!")
        return
    
    # En son dosyayÄ± al
    latest_file = sorted(json_files)[-1]
    filepath = os.path.join(raw_data_dir, latest_file)
    
    print(f"ğŸ“‚ Test dosyasÄ±: {latest_file}")
    
    # Sadece ilk 3 dokÃ¼manÄ± test et
    documents = builder.load_clean_json_data(filepath)[:3]
    split_docs = builder.split_documents(documents)
    
    print(f"\nğŸ“Š CHUNKING TEST SONUÃ‡LARI:")
    print(f"   ğŸ“„ Test dokÃ¼man sayÄ±sÄ±: {len(documents)}")
    print(f"   ğŸ§© OluÅŸan chunk sayÄ±sÄ±: {len(split_docs)}")
    
    print(f"\nğŸ” Ä°LK 3 CHUNK Ã–RNEÄÄ°:")
    for i, chunk in enumerate(split_docs[:3], 1):
        print(f"\n--- CHUNK {i} ---")
        print(f"Kaynak: {chunk.metadata.get('title', 'Bilinmiyor')}")
        print(f"Uzunluk: {len(chunk.page_content)} karakter")
        print(f"Ä°Ã§erik: {chunk.page_content[:200]}...")

def build_full_vectorstore():
    """
    Tam vector store oluÅŸtur
    """
    print("ğŸš€ TAM VECTOR STORE OLUÅTURMA\n")
    
    builder = OptimizedVectorStoreBuilder()
    
    # En son temiz JSON dosyasÄ±nÄ± bul
    raw_data_dir = os.path.join(os.path.dirname(__file__), "..", "data", "raw")
    json_files = [f for f in os.listdir(raw_data_dir) if f.startswith('clean_blog_data_') and f.endswith('.json')]
    
    if not json_files:
        print("âŒ Temiz blog verisi bulunamadÄ±!")
        return None
    
    latest_file = sorted(json_files)[-1]
    filepath = os.path.join(raw_data_dir, latest_file)
    
    print(f"ğŸ“‚ Ä°ÅŸlenecek dosya: {latest_file}")
    vectorstore = builder.process_clean_json_to_vectorstore(filepath)
    
    if vectorstore:
        print("\nâœ… Vector store baÅŸarÄ±yla oluÅŸturuldu!")
        
        # Test arama
        print("\nğŸ” Test aramalarÄ± yapÄ±lÄ±yor...")
        test_queries = [
            "Almanya'da mÃ¼lteci haklarÄ± nelerdir?",
            "Bylock soruÅŸturmalarÄ± hakkÄ±nda ne yazÄ±yor?",
            "Oktay Ã–zdemir kimdir?"
        ]
        
        for query in test_queries:
            print(f"\nâ“ Soru: {query}")
            results = vectorstore.similarity_search(query, k=2)
            for i, result in enumerate(results, 1):
                title = result.metadata.get('title', 'BaÅŸlÄ±k yok')
                print(f"   {i}. {title}")
                print(f"      {result.page_content[:150]}...")
        
        return vectorstore
    else:
        print("âŒ Vector store oluÅŸturulamadÄ±!")
        return None

if __name__ == "__main__":
    print("ğŸ—ï¸  Vector Store Builder - Optimize EdilmiÅŸ\n")
    
    print("SeÃ§enekler:")
    print("1ï¸âƒ£ Test modu - Chunking iÅŸlemini test et")
    print("2ï¸âƒ£ Tam iÅŸlem - Vector store oluÅŸtur\n")
    
    # VarsayÄ±lan olarak test modunu Ã§alÄ±ÅŸtÄ±r
    print("ğŸ”„ Test modu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...\n")
    test_chunking()
    
    print(f"\n{'='*60}")
    print("âœ¨ Test tamamlandÄ±!")
    print("ğŸ’¾ Tam vector store oluÅŸturmak iÃ§in build_full_vectorstore() Ã§alÄ±ÅŸtÄ±rÄ±n.")
