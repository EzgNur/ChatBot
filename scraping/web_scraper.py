"""
Web scraping modÃ¼lÃ¼ - WebBaseLoader ve BeautifulSoup kullanarak
oktayozdemir.com.tr sitesinden veri Ã§ekme
"""

import requests
from bs4 import BeautifulSoup
from langchain_community.document_loaders import WebBaseLoader
import json
import os
from datetime import datetime
from typing import List, Dict

BASE_URL = "https://oktayozdemir.com.tr/"

def get_blog_links() -> List[str]:
    """
    Blog sayfasÄ±ndan tÃ¼m blog yazÄ±sÄ± linklerini Ã§eker
    SayfalandÄ±rma (pagination) desteÄŸi ile
    """
    try:
        all_links = []
        page = 1
        
        while True:
            # Blog sayfasÄ± veya sayfalandÄ±rma
            if page == 1:
                url = BASE_URL + "blog/"
            else:
                url = f"{BASE_URL}blog/page/{page}/"
            
            print(f"ğŸ“„ Sayfa {page} kontrol ediliyor: {url}")
            response = requests.get(url)
            
            # Sayfa bulunamadÄ±ysa dur
            if response.status_code == 404:
                print(f"âŒ Sayfa {page} bulunamadÄ±, tarama tamamlandÄ±.")
                break
                
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            # Bu sayfadaki blog linklerini bul
            page_links = []
            for a in soup.find_all("a", href=True):
                href = a["href"]
                # Daha spesifik filtreleme
                if "/blog/" in href and href not in ["/blog/", "/category/blog/"]:
                    full_url = href if href.startswith("http") else BASE_URL + href.lstrip('/')
                    if full_url not in all_links and full_url not in page_links:
                        page_links.append(full_url)
            
            print(f"âœ… Sayfa {page}'da {len(page_links)} yeni link bulundu")
            
            # Bu sayfada yeni link yoksa dur
            if not page_links:
                print(f"ğŸ”„ Sayfa {page}'da yeni link bulunamadÄ±, tarama tamamlandÄ±.")
                break
                
            all_links.extend(page_links)
            
            # Debug: Ä°lk birkaÃ§ linki gÃ¶ster
            if page == 1 and page_links:
                print("ğŸ” Bulunan linkler Ã¶rneÄŸi:")
                for i, link in enumerate(page_links[:3]):
                    print(f"  {i+1}. {link}")
            
            page += 1
            
            # GÃ¼venlik: Ã‡ok fazla sayfa kontrolÃ¼
            if page > 20:  # Maximum 20 sayfa
                print("âš ï¸  Maksimum sayfa sayÄ±sÄ±na ulaÅŸÄ±ldÄ±.")
                break
        
        print(f"ğŸ¯ Toplam {len(all_links)} blog linki bulundu")
        return all_links
    
    except requests.RequestException as e:
        print(f"âŒ Blog linkleri Ã§ekilirken hata oluÅŸtu: {e}")
        return []

def load_website_docs():
    """
    WebBaseLoader kullanarak ana sayfa ve blog yazÄ±larÄ±nÄ± yÃ¼kler
    """
    try:
        urls = [BASE_URL] + get_blog_links()
        print(f"Toplam {len(urls)} URL yÃ¼kleniyor...")
        
        loader = WebBaseLoader(urls)
        docs = loader.load()
        
        print(f"Toplam {len(docs)} dokÃ¼man yÃ¼klendi")
        return docs
    
    except Exception as e:
        print(f"DokÃ¼manlar yÃ¼klenirken hata oluÅŸtu: {e}")
        return []

def save_raw_data(docs, filename: str = None):
    """
    Ã‡ekilen ham verileri JSON formatÄ±nda kaydeder
    """
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"scraped_data_{timestamp}.json"
    
    # data/raw klasÃ¶rÃ¼nÃ¼ oluÅŸtur
    raw_data_dir = os.path.join(os.path.dirname(__file__), "..", "data", "raw")
    os.makedirs(raw_data_dir, exist_ok=True)
    
    filepath = os.path.join(raw_data_dir, filename)
    
    # DokÃ¼manlarÄ± JSON formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
    data_to_save = []
    for doc in docs:
        data_to_save.append({
            "content": doc.page_content,
            "metadata": doc.metadata,
            "scraped_at": datetime.now().isoformat()
        })
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data_to_save, f, ensure_ascii=False, indent=2)
    
    print(f"Ham veriler kaydedildi: {filepath}")
    return filepath

def scrape_and_save():
    """
    Ana scraping fonksiyonu - verileri Ã§eker ve kaydeder
    """
    print("Web scraping baÅŸlÄ±yor...")
    docs = load_website_docs()
    
    if docs:
        filepath = save_raw_data(docs)
        print(f"Scraping tamamlandÄ±! {len(docs)} dokÃ¼man {filepath} dosyasÄ±na kaydedildi.")
        return docs
    else:
        print("Scraping baÅŸarÄ±sÄ±z oldu!")
        return []

def test_scraping():
    """
    Scraping'i test etmek iÃ§in basit Ã¶nizleme fonksiyonu
    """
    print("ğŸ” Scraping testi baÅŸlÄ±yor...\n")
    
    try:
        docs = load_website_docs()
        print(f"âœ… Toplam {len(docs)} sayfa bulundu.\n")

        for i, doc in enumerate(docs[:5]):  # ilk 5 sayfayÄ± gÃ¶sterelim
            print(f"ğŸ“„ ({i+1}) Kaynak URL: {doc.metadata.get('source', 'Bilinmiyor')}")
            
            # Ä°Ã§erik Ã¶nizleme - TÃ¼rkÃ§e karakterlere dikkat
            content_preview = doc.page_content[:300].replace('\n', ' ').strip()
            print(f"ğŸ“ Ä°Ã§erik Ã¶nizleme: {content_preview}...")
            
            # Metadata bilgileri
            if 'title' in doc.metadata:
                print(f"ğŸ“‹ BaÅŸlÄ±k: {doc.metadata['title']}")
            
            print("-" * 80)
            
        return docs
        
    except Exception as e:
        print(f"âŒ Test sÄ±rasÄ±nda hata: {e}")
        return []

if __name__ == "__main__":
    # Ã–nce test yap
    print("ğŸš€ Scraping modÃ¼lÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...\n")
    print("1ï¸âƒ£ Test modu iÃ§in test_scraping() Ã§alÄ±ÅŸtÄ±rÄ±n")
    print("2ï¸âƒ£ Tam scraping iÃ§in scrape_and_save() Ã§alÄ±ÅŸtÄ±rÄ±n\n")
    
    # Test modunu Ã§alÄ±ÅŸtÄ±r
    docs = test_scraping()
    
    if docs:
        print(f"\nâœ¨ Test baÅŸarÄ±lÄ±! {len(docs)} dokÃ¼man bulundu.")
        print("ğŸ’¾ Verileri kaydetmek iÃ§in scrape_and_save() fonksiyonunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
