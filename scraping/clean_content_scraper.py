"""
Temiz iÃ§erik scraper - Yapay zeka eÄŸitimi iÃ§in optimize edilmiÅŸ
HTML'den gereksiz kÄ±sÄ±mlarÄ± temizler, sadece blog yazÄ±sÄ± iÃ§eriÄŸini alÄ±r
"""

import requests
import xml.etree.ElementTree as ET
import json
import os
import re
from datetime import datetime
from typing import List, Dict
from bs4 import BeautifulSoup, Tag
from langchain.schema import Document

DEFAULT_BASE_URL = "https://oktayozdemir.com.tr/"

class CleanContentScraper:
    def __init__(self, base_url: str | None = None):
        """
        Temiz iÃ§erik scraper sÄ±nÄ±fÄ±
        """
        self.blog_urls = []
        # Taban URL'i dÄ±ÅŸarÄ±dan verilebilir; verilmezse env veya varsayÄ±lan kullanÄ±lÄ±r
        env_base = os.getenv("SCRAPER_BASE_URL")
        self.base_url = (base_url or env_base or DEFAULT_BASE_URL).rstrip('/') + '/'
        
    def get_category_blog_urls(self, category_url: str) -> List[str]:
        """
        Belirli bir kategori sayfasÄ±ndan (ve sayfalandÄ±rmasÄ±ndan) blog yazÄ±sÄ± URL'lerini topla
        Ã–rn: https://oktayozdemir.com.tr/category/almanya-goc-ve-yasam/
        """
        try:
            all_links: List[str] = []
            page = 1
            base = category_url.rstrip('/') + '/'
            
            while True:
                url = base if page == 1 else f"{base}page/{page}/"
                print(f"ğŸ“‚ Kategori sayfasÄ± {page} taranÄ±yor: {url}")
                resp = requests.get(url, timeout=20)
                if resp.status_code == 404:
                    print("âŒ Kategori sayfasÄ± bitti (404)")
                    break
                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                page_links: List[str] = []
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    if '/blog/' in href and href not in ['/blog/', '/category/blog/']:
                        full = href if href.startswith('http') else self.base_url + href.lstrip('/')
                        if full not in all_links and full not in page_links:
                            page_links.append(full)
                
                print(f"âœ… {len(page_links)} link bulundu")
                if not page_links:
                    break
                all_links.extend(page_links)
                page += 1
                if page > 50:
                    print("âš ï¸  GÃ¼venlik iÃ§in 50 sayfa sÄ±nÄ±rÄ±")
                    break
            
            print(f"ğŸ¯ Kategoride toplam {len(all_links)} blog linki bulundu")
            return list(set(all_links))
        except Exception as e:
            print(f"âŒ Kategori linkleri Ã§ekilirken hata: {e}")
            return []
        
    def get_sitemap_urls(self) -> List[str]:
        """
        Sitemap'ten blog URL'lerini al (Ã¶nceki kodla aynÄ±)
        """
        try:
            print("ğŸ” Sitemap index kontrol ediliyor...")
            response = requests.get(self.base_url + "sitemap_index.xml")
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            namespaces = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            sitemap_urls = []
            for sitemap_elem in root.findall('.//sitemap:sitemap', namespaces):
                loc = sitemap_elem.find('sitemap:loc', namespaces)
                if loc is not None:
                    sitemap_urls.append(loc.text)
            
            return sitemap_urls
            
        except Exception as e:
            print(f"âŒ Sitemap index hatasÄ±: {e}")
            return []
    
    def extract_blog_urls_from_sitemap(self, sitemap_url: str) -> List[str]:
        """
        Sitemap'ten blog URL'lerini Ã§Ä±kar
        """
        try:
            response = requests.get(sitemap_url)
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            namespaces = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            blog_urls = []
            for url_elem in root.findall('.//sitemap:url', namespaces):
                loc = url_elem.find('sitemap:loc', namespaces)
                if loc is not None and '/blog/' in loc.text:
                    blog_urls.append(loc.text)
            
            return blog_urls
            
        except Exception as e:
            print(f"âŒ Sitemap iÅŸleme hatasÄ±: {e}")
            return []
    
    def get_all_blog_urls(self) -> List[str]:
        """
        TÃ¼m blog URL'lerini topla
        """
        print("ğŸ“„ Blog URL'leri sitemap'lerden Ã§ekiliyor...")
        
        sitemap_urls = self.get_sitemap_urls()
        if not sitemap_urls:
            return []
        
        all_blog_urls = []
        for sitemap_url in sitemap_urls:
            blog_urls = self.extract_blog_urls_from_sitemap(sitemap_url)
            if blog_urls:
                all_blog_urls.extend(blog_urls)
        
        unique_urls = list(set(all_blog_urls))
        print(f"âœ… {len(unique_urls)} blog yazÄ±sÄ± bulundu")
        return unique_urls

    def get_list_blog_urls(self, list_url: str, max_pages: int | None = None) -> List[str]:
        """
        Blog liste sayfasÄ±ndan (ve sayfalandÄ±rmadan) makale URL'lerini topla.
        Alternativkraft gibi '/tr/blog-2/' yapÄ±larÄ± iÃ§in uygundur.
        """
        try:
            all_links: List[str] = []
            page = 1
            base_list = list_url.rstrip('/') + '/'
            while True:
                url = base_list if page == 1 else f"{base_list}page/{page}/"
                print(f"ğŸ“„ Liste sayfasÄ± {page} taranÄ±yor: {url}")
                resp = requests.get(url, timeout=20)
                if resp.status_code == 404:
                    print("âŒ Liste sayfasÄ± bitti (404)")
                    break
                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, 'html.parser')

                page_links: List[str] = []
                # 'View More' veya baÅŸlÄ±k linkleri
                for a in soup.find_all('a', href=True):
                    text = (a.get_text() or '').strip().lower()
                    href = a['href']
                    if any(key in text for key in ['view more', 'weiterlesen', 'devamÄ±', 'devami']):
                        full = href if href.startswith('http') else self.base_url + href.lstrip('/')
                        if full.startswith(self.base_url) and full not in page_links and 'blog-2' not in full and 'category' not in full:
                            page_links.append(full.split('#')[0])

                for heading in soup.find_all(['h2', 'h3']):
                    a = heading.find('a', href=True)
                    if a:
                        href = a['href']
                        full = href if href.startswith('http') else self.base_url + href.lstrip('/')
                        if full.startswith(self.base_url) and full not in page_links and 'blog-2' not in full and 'category' not in full:
                            page_links.append(full.split('#')[0])

                print(f"âœ… {len(page_links)} link bulundu")
                if not page_links:
                    break
                # benzersiz ekle
                for l in page_links:
                    if l not in all_links:
                        all_links.append(l)
                page += 1
                # SÄ±nÄ±r kontrolÃ¼ (kullanÄ±cÄ± isteÄŸi)
                if max_pages is not None and page > max_pages:
                    print(f"â¹ï¸  Sayfa sÄ±nÄ±rÄ±na ulaÅŸÄ±ldÄ±: max_pages={max_pages}")
                    break
                if page > 100:
                    print("âš ï¸  GÃ¼venlik iÃ§in 100 sayfa sÄ±nÄ±rÄ±")
                    break

            print(f"ğŸ¯ Listeden toplam {len(all_links)} makale linki bulundu")
            return all_links
        except Exception as e:
            print(f"âŒ Liste linkleri Ã§ekilirken hata: {e}")
            return []
    
    def clean_html_content(self, html_content: str) -> Dict[str, str]:
        """
        HTML'den temiz blog iÃ§eriÄŸini Ã§Ä±kar
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # BaÅŸlÄ±k Ã§Ä±kar
        title = ""
        title_selectors = [
            'h1.entry-title',
            '.post-title h1',
            'h1',
            '.entry-header h1',
            'title'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                # Site adÄ±nÄ± temizle
                title = title.replace(' - Oktay Ã–zdemir DanÄ±ÅŸmanlÄ±k', '')
                break
        
        # Ana iÃ§erik Ã§Ä±kar
        content = ""
        content_selectors = [
            '.entry-content',
            '.post-content', 
            'article .content',
            '.blog-post-content',
            'main article',
            '.single-post-content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # Gereksiz elementleri kaldÄ±r
                for unwanted in content_elem.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form']):
                    unwanted.decompose()
                
                # Reklam ve sosyal medya butonlarÄ±nÄ± kaldÄ±r
                for unwanted_class in ['.social-share', '.advertisement', '.ads', '.sidebar', '.related-posts']:
                    for elem in content_elem.select(unwanted_class):
                        elem.decompose()
                
                content = content_elem.get_text()
                break
        
        # Ä°Ã§erik temizleme
        if content:
            # Fazla boÅŸluklarÄ± temizle
            content = re.sub(r'\n\s*\n', '\n\n', content)  # Ã‡oklu satÄ±r sonlarÄ±nÄ± dÃ¼zenle
            content = re.sub(r' +', ' ', content)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
            content = content.strip()
            
            # Ã‡ok kÄ±sa iÃ§eriÄŸi filtrele
            if len(content) < 200:
                content = ""
        
        # Tarih Ã§Ä±kar
        date = ""
        date_selectors = [
            '.entry-date',
            '.post-date',
            'time[datetime]',
            '.published'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date = date_elem.get_text().strip() or date_elem.get('datetime', '')
                break
        
        # Yazar bilgisi
        author = "Oktay Ã–zdemir"  # VarsayÄ±lan
        author_selectors = [
            '.author-name',
            '.post-author',
            '.entry-author'
        ]
        
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                author = author_elem.get_text().strip()
                break
        
        return {
            'title': title,
            'content': content,
            'author': author,
            'date': date,
            'word_count': len(content.split()) if content else 0
        }
    
    def scrape_single_blog(self, url: str) -> Dict[str, str]:
        """
        Tek bir blog yazÄ±sÄ±nÄ± temiz formatta Ã§ek
        """
        try:
            response = requests.get(url)
            response.raise_for_status()
            
            cleaned_data = self.clean_html_content(response.text)
            cleaned_data['url'] = url
            
            return cleaned_data
            
        except Exception as e:
            print(f"âŒ Blog Ã§ekme hatasÄ± ({url}): {e}")
            return {'url': url, 'title': '', 'content': '', 'author': '', 'date': '', 'word_count': 0}
    
    def scrape_all_blogs(self) -> List[Dict[str, str]]:
        """
        TÃ¼m blog yazÄ±larÄ±nÄ± temiz formatta Ã§ek
        """
        print("ğŸš€ Temiz iÃ§erik scraping baÅŸlÄ±yor...")
        
        blog_urls = self.get_all_blog_urls()
        if not blog_urls:
            print("âŒ Blog URL'si bulunamadÄ±!")
            return []
        
        print(f"ğŸ“¥ {len(blog_urls)} blog yazÄ±sÄ± iÅŸlenecek...")
        
        all_blogs = []
        successful_count = 0
        
        for i, url in enumerate(blog_urls, 1):
            print(f"â³ ({i}/{len(blog_urls)}) Ä°ÅŸleniyor: {url.split('/')[-1]}")
            
            blog_data = self.scrape_single_blog(url)
            
            if blog_data['content'] and len(blog_data['content']) > 200:
                all_blogs.append(blog_data)
                successful_count += 1
                print(f"   âœ… BaÅŸarÄ±lÄ± - {blog_data['word_count']} kelime")
            else:
                print(f"   âš ï¸  Ä°Ã§erik Ã§ok kÄ±sa veya bulunamadÄ±")
        
        print(f"\nğŸ¯ Toplam {successful_count}/{len(blog_urls)} blog yazÄ±sÄ± baÅŸarÄ±yla iÅŸlendi!")
        return all_blogs

    def scrape_list_blogs(self, list_url: str, max_pages: int | None = None) -> List[Dict[str, str]]:
        """
        Liste sayfasÄ±ndaki tÃ¼m makaleleri temiz formatta Ã§ek.
        """
        print("ğŸš€ Liste bazlÄ± temiz iÃ§erik scraping baÅŸlÄ±yor...")
        urls = self.get_list_blog_urls(list_url, max_pages=max_pages)
        if not urls:
            print("âŒ Liste URL'sinden makale linki bulunamadÄ±!")
            return []
        print(f"ğŸ“¥ {len(urls)} makale iÅŸlenecek...")
        results: List[Dict[str, str]] = []
        for i, url in enumerate(urls, 1):
            print(f"â³ ({i}/{len(urls)}) {url}")
            data = self.scrape_single_blog(url)
            if data['content'] and len(data['content']) > 200:
                results.append(data)
                print(f"   âœ… {data['word_count']} kelime")
            else:
                print("   âš ï¸  Ä°Ã§erik kÄ±sa/boÅŸ")
        print(f"\nğŸ¯ Listeden {len(results)}/{len(urls)} yazÄ± baÅŸarÄ±yla iÅŸlendi!")
        return results

    def scrape_category_blogs(self, category_url: str) -> List[Dict[str, str]]:
        """
        Sadece verilen kategori altÄ±ndaki blog yazÄ±larÄ±nÄ± temiz formatta Ã§ek
        """
        print("ğŸš€ Kategori bazlÄ± temiz iÃ§erik scraping baÅŸlÄ±yor...")
        cat_urls = self.get_category_blog_urls(category_url)
        if not cat_urls:
            print("âŒ Kategori iÃ§inde blog URL'si bulunamadÄ±!")
            return []
        print(f"ğŸ“¥ {len(cat_urls)} blog yazÄ±sÄ± iÅŸlenecek...")
        results: List[Dict[str, str]] = []
        for i, url in enumerate(cat_urls, 1):
            print(f"â³ ({i}/{len(cat_urls)}) {url}")
            data = self.scrape_single_blog(url)
            if data['content'] and len(data['content']) > 200:
                results.append(data)
                print(f"   âœ… {data['word_count']} kelime")
            else:
                print("   âš ï¸  Ä°Ã§erik kÄ±sa/boÅŸ")
        print(f"\nğŸ¯ Kategoride {len(results)}/{len(cat_urls)} yazÄ± baÅŸarÄ±yla iÅŸlendi!")
        return results
    
    def save_clean_data(self, blogs_data: List[Dict], filename: str = None):
        """
        Temiz verileri kaydet - Yapay zeka eÄŸitimi iÃ§in optimize edilmiÅŸ format
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"clean_blog_data_{timestamp}.json"
        
        # data/raw klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        raw_data_dir = os.path.join(os.path.dirname(__file__), "..", "data", "raw")
        os.makedirs(raw_data_dir, exist_ok=True)
        
        filepath = os.path.join(raw_data_dir, filename)
        
        # Yapay zeka eÄŸitimi iÃ§in optimize edilmiÅŸ format
        ai_training_data = []
        
        for blog in blogs_data:
            if blog['content']:  # Sadece iÃ§eriÄŸi olan yazÄ±larÄ± ekle
                ai_training_data.append({
                    "id": len(ai_training_data) + 1,
                    "title": blog['title'],
                    "content": blog['content'],
                    "author": blog['author'],
                    "date": blog['date'],
                    "url": blog['url'],
                    "word_count": blog['word_count'],
                    "scraped_at": datetime.now().isoformat(),
                    "scraper_type": "clean_content",
                    # Yapay zeka iÃ§in ek metadata
                    "content_type": "blog_post",
                    "language": "tr",
                    "domain": "law_immigration_politics"
                })
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(ai_training_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Temiz veriler kaydedildi: {filepath}")
        print(f"ğŸ“Š Kaydedilen yazÄ± sayÄ±sÄ±: {len(ai_training_data)}")
        print(f"ğŸ“ Toplam kelime sayÄ±sÄ±: {sum(blog['word_count'] for blog in ai_training_data):,}")
        
        return filepath

def test_clean_scraping():
    """
    Temiz scraping'i test et
    """
    print("ğŸ” Temiz iÃ§erik testi baÅŸlÄ±yor...\n")
    
    scraper = CleanContentScraper()
    
    # Sadece 3 blog yazÄ±sÄ±nÄ± test et
    blog_urls = scraper.get_all_blog_urls()
    if not blog_urls:
        print("âŒ Test iÃ§in blog URL'si bulunamadÄ±!")
        return
    
    test_urls = blog_urls[:3]
    print(f"ğŸ“¥ Test iÃ§in {len(test_urls)} blog yazÄ±sÄ± iÅŸlenecek...\n")
    
    for i, url in enumerate(test_urls, 1):
        print(f"ğŸ”„ Test {i}: {url.split('/')[-1]}")
        
        blog_data = scraper.scrape_single_blog(url)
        
        print(f"   ğŸ“‹ BaÅŸlÄ±k: {blog_data['title']}")
        print(f"   âœï¸  Yazar: {blog_data['author']}")
        print(f"   ğŸ“… Tarih: {blog_data['date']}")
        print(f"   ğŸ“ Kelime sayÄ±sÄ±: {blog_data['word_count']}")
        
        if blog_data['content']:
            preview = blog_data['content'][:300].replace('\n', ' ')
            print(f"   ğŸ” Ä°Ã§erik Ã¶nizleme: {preview}...")
        else:
            print(f"   âš ï¸  Ä°Ã§erik bulunamadÄ±")
        
        print("-" * 80)
    
    print("âœ… Temiz iÃ§erik testi tamamlandÄ±!")

def clean_scrape_and_save(base_url: str | None = None):
    """
    Ana temiz scraping fonksiyonu
    """
    scraper = CleanContentScraper(base_url=base_url)
    blogs_data = scraper.scrape_all_blogs()
    
    if blogs_data:
        filepath = scraper.save_clean_data(blogs_data)
        print(f"ğŸ‰ Temiz scraping tamamlandÄ±! Veriler {filepath} dosyasÄ±na kaydedildi.")
        return blogs_data
    else:
        print("âŒ Temiz scraping baÅŸarÄ±sÄ±z!")
        return []

def clean_scrape_category_and_save(category_url: str, base_url: str | None = None):
    """
    Verilen kategori URL'sinden temiz iÃ§erik Ã§ek ve kaydet
    """
    scraper = CleanContentScraper(base_url=base_url)
    blogs = scraper.scrape_category_blogs(category_url)
    if blogs:
        filepath = scraper.save_clean_data(blogs)
        print(f"ğŸ‰ Kategori scraping tamamlandÄ±! Veriler {filepath} dosyasÄ±na kaydedildi.")
        return blogs
    else:
        print("âŒ Kategori scraping baÅŸarÄ±sÄ±z!")
        return []

def clean_scrape_list_and_save(list_url: str, base_url: str | None = None, max_pages: int | None = None):
    """
    Verilen liste URL'sinden (Ã¶r. https://alternativkraft.com/tr/blog-2/) temiz iÃ§erik Ã§ek ve kaydet
    """
    scraper = CleanContentScraper(base_url=base_url)
    blogs = scraper.scrape_list_blogs(list_url, max_pages=max_pages)
    if blogs:
        filepath = scraper.save_clean_data(blogs)
        print(f"ğŸ‰ Liste scraping tamamlandÄ±! Veriler {filepath} dosyasÄ±na kaydedildi.")
        return blogs
    else:
        print("âŒ Liste scraping baÅŸarÄ±sÄ±z!")
        return []

if __name__ == "__main__":
    print("ğŸ§¹ Temiz Ä°Ã§erik Scraper - Yapay Zeka EÄŸitimi iÃ§in Optimize EdilmiÅŸ\n")
    
    print("SeÃ§enekler:")
    print("1ï¸âƒ£ Test modu - 3 blog yazÄ±sÄ±nÄ±n temiz formatÄ±nÄ± gÃ¶ster")
    print("2ï¸âƒ£ Tam scraping - TÃ¼m blog yazÄ±larÄ±nÄ± temiz formatta Ã§ek ve kaydet\n")
    
    # Test modunu Ã§alÄ±ÅŸtÄ±r
    print("ğŸ”„ Test modu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...\n")
    test_clean_scraping()
