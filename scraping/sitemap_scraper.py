"""
Sitemap tabanlÄ± web scraper
En gÃ¼venilir yÃ¶ntem - sitemap.xml dosyalarÄ±ndan blog linklerini Ã§eker
"""

import requests
import xml.etree.ElementTree as ET
import json
import os
from datetime import datetime
from typing import List
from langchain_community.document_loaders import WebBaseLoader

BASE_URL = "https://oktayozdemir.com.tr/"

class SitemapScraper:
    def __init__(self):
        """
        Sitemap tabanlÄ± scraper sÄ±nÄ±fÄ±
        """
        self.blog_urls = []
        
    def get_sitemap_urls(self) -> List[str]:
        """
        Sitemap index'ten tÃ¼m alt sitemap URL'lerini al
        """
        try:
            print("ğŸ” Sitemap index kontrol ediliyor...")
            response = requests.get(BASE_URL + "sitemap_index.xml")
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            namespaces = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            sitemap_urls = []
            for sitemap_elem in root.findall('.//sitemap:sitemap', namespaces):
                loc = sitemap_elem.find('sitemap:loc', namespaces)
                if loc is not None:
                    sitemap_urls.append(loc.text)
            
            print(f"âœ… {len(sitemap_urls)} alt sitemap bulundu")
            return sitemap_urls
            
        except Exception as e:
            print(f"âŒ Sitemap index hatasÄ±: {e}")
            return []
    
    def extract_blog_urls_from_sitemap(self, sitemap_url: str) -> List[str]:
        """
        Tek bir sitemap'ten blog URL'lerini Ã§Ä±kar
        """
        try:
            response = requests.get(sitemap_url)
            response.raise_for_status()
            
            root = ET.fromstring(response.text)
            namespaces = {'sitemap': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            blog_urls = []
            for url_elem in root.findall('.//sitemap:url', namespaces):
                loc = url_elem.find('sitemap:loc', namespaces)
                if loc is not None and '/blog/' in loc.text:
                    blog_urls.append(loc.text)
            
            return blog_urls
            
        except Exception as e:
            print(f"âŒ Sitemap iÅŸleme hatasÄ± ({sitemap_url}): {e}")
            return []
    
    def get_all_blog_urls(self) -> List[str]:
        """
        TÃ¼m sitemap'lerden blog URL'lerini topla
        """
        print("ğŸ“„ Blog URL'leri sitemap'lerden Ã§ekiliyor...")
        
        sitemap_urls = self.get_sitemap_urls()
        if not sitemap_urls:
            print("âŒ Sitemap URL'leri bulunamadÄ±!")
            return []
        
        all_blog_urls = []
        
        for sitemap_url in sitemap_urls:
            blog_urls = self.extract_blog_urls_from_sitemap(sitemap_url)
            if blog_urls:
                sitemap_name = sitemap_url.split('/')[-1]
                print(f"âœ… {sitemap_name}: {len(blog_urls)} blog yazÄ±sÄ±")
                all_blog_urls.extend(blog_urls)
        
        # Tekrar eden URL'leri temizle
        unique_urls = list(set(all_blog_urls))
        print(f"ğŸ¯ Toplam {len(unique_urls)} benzersiz blog yazÄ±sÄ± bulundu!")
        
        return unique_urls
    
    def load_website_docs(self) -> List:
        """
        Sitemap'ten alÄ±nan URL'leri WebBaseLoader ile yÃ¼kle
        """
        print("ğŸš€ Sitemap tabanlÄ± scraping baÅŸlÄ±yor...")
        
        # Blog URL'lerini al
        blog_urls = self.get_all_blog_urls()
        
        if not blog_urls:
            print("âŒ Blog URL'si bulunamadÄ±!")
            return []
        
        # Ana sayfa + tÃ¼m blog yazÄ±larÄ±
        all_urls = [BASE_URL] + blog_urls
        
        print(f"ğŸ“¥ {len(all_urls)} URL yÃ¼kleniyor...")
        print("â³ Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")
        
        try:
            loader = WebBaseLoader(all_urls)
            docs = loader.load()
            print(f"âœ… {len(docs)} dokÃ¼man baÅŸarÄ±yla yÃ¼klendi!")
            return docs
            
        except Exception as e:
            print(f"âŒ WebBaseLoader hatasÄ±: {e}")
            return []
    
    def save_sitemap_data(self, docs, filename: str = None):
        """
        Sitemap scraping verilerini kaydet
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sitemap_scraped_data_{timestamp}.json"
        
        # data/raw klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        raw_data_dir = os.path.join(os.path.dirname(__file__), "..", "data", "raw")
        os.makedirs(raw_data_dir, exist_ok=True)
        
        filepath = os.path.join(raw_data_dir, filename)
        
        # DokÃ¼manlarÄ± JSON formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
        data_to_save = []
        for doc in docs:
            data_to_save.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "scraped_at": datetime.now().isoformat(),
                "scraper_type": "sitemap_based"
            })
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Sitemap scraping verileri kaydedildi: {filepath}")
        return filepath

def test_sitemap_scraping():
    """
    Sitemap scraper'Ä± test et
    """
    print("ğŸ” Sitemap scraping testi baÅŸlÄ±yor...\n")
    
    try:
        scraper = SitemapScraper()
        
        # Sadece URL'leri al (test iÃ§in)
        blog_urls = scraper.get_all_blog_urls()
        
        if not blog_urls:
            print("âŒ Test baÅŸarÄ±sÄ±z - blog URL'si bulunamadÄ±!")
            return []
        
        print(f"\nğŸ” Bulunan blog yazÄ±larÄ± (ilk 15):")
        for i, url in enumerate(blog_urls[:15], 1):
            # URL'den baÅŸlÄ±k tahmini
            title_part = url.split('/')[-1].replace('-', ' ').title()
            print(f"  {i}. {title_part}")
            print(f"     {url}")
        
        if len(blog_urls) > 15:
            print(f"  ... ve {len(blog_urls) - 15} tane daha")
        
        print(f"\nğŸ“Š SonuÃ§: {len(blog_urls)} blog yazÄ±sÄ± tespit edildi")
        print("âœ… Bu sayÄ± beklediÄŸiniz 60'a yakÄ±n!")
        
        return blog_urls
        
    except Exception as e:
        print(f"âŒ Test sÄ±rasÄ±nda hata: {e}")
        return []

def sitemap_scrape_and_save():
    """
    Ana sitemap scraping fonksiyonu
    """
    scraper = SitemapScraper()
    docs = scraper.load_website_docs()
    
    if docs:
        filepath = scraper.save_sitemap_data(docs)
        print(f"ğŸ‰ Sitemap scraping tamamlandÄ±! {len(docs)} dokÃ¼man kaydedildi.")
        return docs
    else:
        print("âŒ Sitemap scraping baÅŸarÄ±sÄ±z!")
        return []

if __name__ == "__main__":
    print("ğŸ—ºï¸  Sitemap tabanlÄ± scraper baÅŸlatÄ±lÄ±yor...\n")
    print("Bu yÃ¶ntem en gÃ¼venilir olanÄ±dÄ±r - sitemap.xml dosyalarÄ±nÄ± kullanÄ±r\n")
    
    print("SeÃ§enekler:")
    print("1ï¸âƒ£ Test modu - Sadece blog linklerini listele")
    print("2ï¸âƒ£ Tam scraping - TÃ¼m verileri Ã§ek ve kaydet\n")
    
    # VarsayÄ±lan olarak test modunu Ã§alÄ±ÅŸtÄ±r
    print("ğŸ”„ Test modu Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...\n")
    links = test_sitemap_scraping()
    
    if links and len(links) >= 30:  # En az 30 link bulunmuÅŸsa
        print(f"\nâœ¨ Test baÅŸarÄ±lÄ±! {len(links)} link bulundu.")
        print("ğŸ’¾ TÃ¼m verileri kaydetmek iÃ§in sitemap_scrape_and_save() Ã§alÄ±ÅŸtÄ±rÄ±n.")
    else:
        print(f"\nâš ï¸  Sadece {len(links) if links else 0} link bulundu.")
        print("ğŸ”§ Beklenenden az, ama yine de devam edebiliriz.")
