"""
BirleÅŸik ingest komutu

KullanÄ±m Ã¶rnekleri:
  - TÃ¼m akÄ±ÅŸ (temiz scraping + vectorstore'u baÅŸtan kur):
      python3 ingest.py --mode clean --rebuild

  - YalnÄ±zca incremental ekleme (mevcut FAISS'e yeni temiz JSON'u ekle):
      python3 ingest.py --mode clean --incremental

  - Belirli bir temiz JSON dosyasÄ±nÄ± incremental ekle:
      python3 ingest.py --json data/raw/clean_blog_data_20250101_120000.json --incremental

Notlar:
  - VarsayÄ±lan kÃ¶k: bu dosyanÄ±n bulunduÄŸu proje dizini
  - Embedding modeli, mevcut yapÄ±yla uyumlu: paraphrase-multilingual-MiniLM-L12-v2
"""

import argparse
import glob
import os
import sys
from typing import Optional

# Proje kÃ¶kÃ¼nÃ¼ PYTHONPATH'e ekle (bu dosya artÄ±k scraping/ altÄ±nda)
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from scraping.clean_content_scraper import (
    clean_scrape_and_save,
    clean_scrape_category_and_save,
    clean_scrape_list_and_save,
)  # type: ignore
from scraping.sitemap_scraper import sitemap_scrape_and_save  # type: ignore
from vectorstore.build_store import OptimizedVectorStoreBuilder, build_full_vectorstore  # type: ignore


def find_latest_clean_json(root: str) -> Optional[str]:
    raw_dir = os.path.join(root, "data", "raw")
    if not os.path.exists(raw_dir):
        return None
    files = sorted(glob.glob(os.path.join(raw_dir, "clean_blog_data_*.json")))
    return files[-1] if files else None


def run_clean_scrape(root: str) -> Optional[str]:
    """Temiz scraping Ã§alÄ±ÅŸtÄ±rÄ±r ve oluÅŸturulan dosya yolunu dÃ¶ner."""
    blogs = clean_scrape_and_save()
    if blogs is None:
        return None
    return find_latest_clean_json(root)


def run_sitemap_scrape(root: str) -> Optional[str]:
    """Sitemap scraping Ã§alÄ±ÅŸtÄ±rÄ±r (ham veri Ã¼retir). Bu veri doÄŸrudan builder ile uyumlu deÄŸildir."""
    docs = sitemap_scrape_and_save()
    if not docs:
        return None
    # KullanÄ±cÄ±ya temiz scraper'Ä± Ã¶nermeye devam ediyoruz, ama yine de en gÃ¼ncel clean dosyayÄ± dÃ¶ndÃ¼rmeye Ã§alÄ±ÅŸalÄ±m
    return find_latest_clean_json(root)

def run_list_scrape(root: str, list_url: str, base_url: Optional[str], max_pages: Optional[int]) -> Optional[str]:
    """Liste URL Ã¼zerinden scraping Ã§alÄ±ÅŸtÄ±r ve oluÅŸan temiz JSON yolunu dÃ¶ndÃ¼r."""
    blogs = clean_scrape_list_and_save(list_url, base_url=base_url, max_pages=max_pages)
    if not blogs:
        return None
    return find_latest_clean_json(root)


def incremental_ingest(root: str, json_path: str) -> None:
    builder = OptimizedVectorStoreBuilder()

    # 1) Mevcut FAISS'i yÃ¼kle (yoksa oluÅŸturacaÄŸÄ±z)
    vs_path = os.path.join(root, "data", "vectorstore")
    vectorstore = builder.load_vectorstore(vs_path)

    # EÄŸer yoksa yeni oluÅŸturmak iÃ§in boÅŸ bir kurulum yapalÄ±m
    if vectorstore is None:
        # Ä°lk kurulum: json'dan doÄŸrudan process ederek oluÅŸtur
        print("âš ï¸ Mevcut FAISS bulunamadÄ±. Ä°lk kez oluÅŸturulacak.")
        vectorstore = builder.process_clean_json_to_vectorstore(json_path)
        return

    # 2) Yeni veriyi yÃ¼kle, chunk'la ve ekle
    documents = builder.load_clean_json_data(json_path)
    if not documents:
        print("âŒ JSON verisi yÃ¼klenemedi veya boÅŸ")
        return

    split_docs = builder.split_documents(documents)
    vectorstore.add_documents(split_docs)

    # 3) Kaydet
    vectorstore.save_local(vs_path)
    print("âœ… Incremental ingest tamamlandÄ± ve mevcut FAISS kaydedildi.")


def main():
    parser = argparse.ArgumentParser(description="BirleÅŸik ingest aracÄ±")
    parser.add_argument("--mode", choices=["clean", "sitemap"], default="clean",
                        help="Veri Ã§ekme modu: clean (Ã¶nerilen) veya sitemap")
    parser.add_argument("--rebuild", action="store_true", help="Vectorstore'u baÅŸtan kur")
    parser.add_argument("--incremental", action="store_true", help="Mevcut FAISS'e ekleme yap")
    parser.add_argument("--json", type=str, default=None, help="Belirli bir temiz JSON dosyasÄ± yolu")
    parser.add_argument("--category", type=str, default=None, help="Kategori URL'si (Ã¶rn: https://oktayozdemir.com.tr/category/almanya-goc-ve-yasam/)")
    parser.add_argument("--review", action="store_true", help="Ä°ÅŸlenecek veriyi FAISS'e eklemeden Ã¶nce Ã¶zet ve Ã¶nizleme gÃ¶ster, onay iste")
    parser.add_argument("--show-full", action="store_true", help="Ã–nizlemede iÃ§eriklerin tamamÄ±nÄ± gÃ¶ster (uzun olabilir)")
    parser.add_argument("--base-url", type=str, default=None, help="Scraper taban URL (Ã¶rn: https://alternativkraft.com/)")
    parser.add_argument("--list-url", type=str, default=None, help="Blog liste URL'si (Ã¶rn: https://alternativkraft.com/tr/blog-2/)")
    parser.add_argument("--scrape-only", action="store_true", help="Sadece scraping yap ve JSON'u Ã¼ret, vectorstore iÅŸlemi yapma")
    parser.add_argument("--max-pages", type=int, default=None, help="Liste sayfasÄ± iÃ§in maksimum sayfa sayÄ±sÄ±")
    parser.add_argument("--per-article", action="store_true", help="Ä°nceleme modunda her makale iÃ§in tek tek onay iste")

    args = parser.parse_args()
    root = PROJECT_ROOT

    # 1) JSON kaynaÄŸÄ±nÄ± hazÄ±rla
    json_path: Optional[str] = args.json
    if json_path is None:
        if args.list_url:
            print(f"ğŸ“š Liste scraping: {args.list_url}")
            json_path = run_list_scrape(root, args.list_url, base_url=args.base_url, max_pages=args.max_pages)
        elif args.category:
            print(f"ğŸ“š Kategori scraping: {args.category}")
            blogs = clean_scrape_category_and_save(args.category, base_url=args.base_url)
            json_path = find_latest_clean_json(root)
        elif args.mode == "clean":
            # base_url desteÄŸi ile Ã§alÄ±ÅŸtÄ±r
            blogs = clean_scrape_and_save(base_url=args.base_url)
            json_path = find_latest_clean_json(root)
        else:
            json_path = run_sitemap_scrape(root)

    if not json_path or not os.path.exists(json_path):
        print("âŒ Temiz JSON dosyasÄ± bulunamadÄ±. --json ile yol verebilir veya --mode clean kullanabilirsiniz.")
        sys.exit(1)

    print(f"ğŸ“„ KullanÄ±lacak JSON: {os.path.relpath(json_path, root)}")

    if args.scrape_only:
        print("â¹ï¸  --scrape-only seÃ§ildi. Sadece scraping yapÄ±ldÄ±, vectorstore iÅŸlemi yapÄ±lmayacak.")
        return

    # 1.5) Ä°nceleme modu: KullanÄ±cÄ± onayÄ± almadan FAISS'e ekleme
    if args.review:
        try:
            import json as _json
            from textwrap import fill as _fill
            with open(json_path, 'r', encoding='utf-8') as f:
                data = _json.load(f)
        except Exception as e:
            print(f"âŒ JSON okunamadÄ±: {e}")
            sys.exit(1)

        n = len(data) if isinstance(data, list) else 0
        total_words = sum(int(x.get('word_count', 0) or 0) for x in data) if n else 0
        print("\n=== Ä°NCELEME Ã–ZETÄ° ===")
        print(f"Toplam kayÄ±t: {n}")
        print(f"Toplam kelime: {total_words:,}")

        if not args.per_article:
            preview_limit = n if args.show_full else min(10, n)
            if n:
                print("\n--- Ã–NÄ°ZLEME ---")
                for i, x in enumerate(data[:preview_limit], 1):
                    title = x.get('title', '')
                    url = x.get('url', '')
                    content = x.get('content', '') or ''
                    snippet = content if args.show_full else content[:600]
                    snippet = snippet.replace('\n', ' ')
                    print(f"\n[{i}] {title}\nURL: {url}")
                    print(_fill(snippet, width=100))
                if not args.show_full and n > preview_limit:
                    print(f"\n(Toplam {n} kayÄ±ttan ilk {preview_limit} gÃ¶sterildi. TamamÄ± iÃ§in --show-full ekleyin.)")

            try:
                decision = input("\nBu verileri FAISS'e eklemek istiyor musunuz? [y/N]: ").strip().lower()
            except EOFError:
                decision = 'n'
            if decision not in ('y', 'yes'):
                print("â¹ï¸  Ä°ÅŸlem iptal edildi (FAISS'e ekleme yapÄ±lmadÄ±).")
                return
        else:
            # Per-article review: kullanÄ±cÄ± her makale iÃ§in onay verir
            print("\n--- PER-ARTÄ°CLE Ä°NCELEME ---")
            approved: list[dict] = []
            for idx, x in enumerate(data, 1):
                title = x.get('title', '')
                url = x.get('url', '')
                content = x.get('content', '') or ''
                snippet = content if args.show_full else content[:600]
                snippet = snippet.replace('\n', ' ')
                print(f"\n[{idx}/{n}] {title}\nURL: {url}")
                print(_fill(snippet, width=100))
                try:
                    decision = input("Ekle? [y/N]: ").strip().lower()
                except EOFError:
                    decision = 'n'
                if decision in ('y', 'yes'):
                    approved.append(x)
            if not approved:
                print("â¹ï¸  HiÃ§ bir makale onaylanmadÄ±. Ä°ÅŸlem durduruldu.")
                return
            # OnaylananlarÄ± yeni geÃ§ici JSON'a yaz
            import tempfile, json as _json
            fd, tmp_path = tempfile.mkstemp(prefix="approved_", suffix=".json")
            os.close(fd)
            with open(tmp_path, 'w', encoding='utf-8') as f:
                _json.dump(approved, f, ensure_ascii=False, indent=2)
            print(f"âœ… Onaylanan {len(approved)} makale geÃ§ici dosyaya yazÄ±ldÄ±: {tmp_path}")
            json_path = tmp_path

    # 2) Vectorstore iÅŸlemi
    if args.rebuild:
        print("ğŸš€ Rebuild modu: vectorstore baÅŸtan oluÅŸturuluyor...")
        builder = OptimizedVectorStoreBuilder()
        builder.process_clean_json_to_vectorstore(json_path)
        print("âœ… Rebuild tamamlandÄ±.")
        return

    if args.incremental:
        print("â• Incremental ingest baÅŸlÄ±yor...")
        incremental_ingest(root, json_path)
        return

    # VarsayÄ±lan davranÄ±ÅŸ: rebuild
    print("â„¹ï¸  Ne --rebuild ne de --incremental seÃ§ildi. VarsayÄ±lan olarak rebuild yapÄ±lÄ±yor...")
    builder = OptimizedVectorStoreBuilder()
    builder.process_clean_json_to_vectorstore(json_path)
    print("âœ… Tam iÅŸlem tamamlandÄ±.")


if __name__ == "__main__":
    main()


