"""
Selenium tabanlÄ± geliÅŸmiÅŸ web scraper
Dinamik iÃ§erik ve AJAX yÃ¼klenen blog yazÄ±larÄ±nÄ± Ã§eker
"""

import time
import json
import os
from datetime import datetime
from typing import List, Set
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from langchain_community.document_loaders import WebBaseLoader

BASE_URL = "https://oktayozdemir.com.tr/"

class AdvancedScraper:
    def __init__(self, headless=True):
        """
        GeliÅŸmiÅŸ scraper sÄ±nÄ±fÄ±
        """
        self.headless = headless
        self.driver = None
        self.blog_links = set()
        
    def setup_driver(self):
        """
        Chrome WebDriver'Ä± kur
        """
        chrome_options = Options()
        if self.headless:
            chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # User agent ekle
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        print("âœ… Chrome WebDriver baÅŸlatÄ±ldÄ±")
    
    def scroll_and_load_more(self, url: str, max_scrolls: int = 10) -> Set[str]:
        """
        SayfayÄ± aÅŸaÄŸÄ± kaydÄ±rarak dinamik iÃ§erik yÃ¼kler
        """
        print(f"ğŸ”„ Dinamik iÃ§erik yÃ¼kleniyor: {url}")
        self.driver.get(url)
        
        # Sayfa yÃ¼klenene kadar bekle
        WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        found_links = set()
        scroll_count = 0
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        
        while scroll_count < max_scrolls:
            # Mevcut linkleri topla
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            page_links = self.extract_blog_links(soup)
            
            new_links = page_links - found_links
            if new_links:
                found_links.update(new_links)
                print(f"ğŸ“„ Scroll {scroll_count + 1}: {len(new_links)} yeni link bulundu (Toplam: {len(found_links)})")
            
            # SayfayÄ± aÅŸaÄŸÄ± kaydÄ±r
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            
            # "Load More" butonu varsa tÄ±kla
            try:
                load_more_btn = self.driver.find_element(By.XPATH, 
                    "//button[contains(text(), 'Load More') or contains(text(), 'Daha Fazla') or contains(text(), 'DevamÄ±')]")
                if load_more_btn.is_displayed():
                    load_more_btn.click()
                    print("ğŸ”„ 'Load More' butonuna tÄ±klandÄ±")
                    time.sleep(2)
            except NoSuchElementException:
                pass
            
            # Yeni iÃ§erik yÃ¼klenene kadar bekle
            time.sleep(3)
            
            # Sayfa boyutu deÄŸiÅŸti mi kontrol et
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                print("ğŸ”„ Daha fazla iÃ§erik yÃ¼klenemedi")
                break
                
            last_height = new_height
            scroll_count += 1
        
        print(f"âœ… Toplam {len(found_links)} benzersiz blog linki bulundu")
        return found_links
    
    def extract_blog_links(self, soup: BeautifulSoup) -> Set[str]:
        """
        BeautifulSoup ile blog linklerini Ã§Ä±kar
        """
        links = set()
        
        # FarklÄ± selektÃ¶rleri dene
        selectors = [
            'a[href*="/blog/"]',
            '.post-title a',
            '.entry-title a',
            'article a',
            '.blog-post a',
            'h1 a, h2 a, h3 a'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for element in elements:
                href = element.get('href')
                if href and '/blog/' in href and href not in ['/blog/', '/category/blog/']:
                    full_url = href if href.startswith('http') else BASE_URL + href.lstrip('/')
                    # GerÃ§ek blog yazÄ±sÄ± linklerini filtrele
                    if self.is_valid_blog_link(full_url):
                        links.add(full_url)
        
        return links
    
    def is_valid_blog_link(self, url: str) -> bool:
        """
        GeÃ§erli blog yazÄ±sÄ± linki mi kontrol et
        """
        invalid_patterns = [
            '/blog/',
            '/category/',
            '/tag/',
            '/page/',
            '/author/',
            '#',
            'javascript:',
            'mailto:'
        ]
        
        # GeÃ§ersiz pattern'larÄ± kontrol et
        for pattern in invalid_patterns:
            if url.endswith(pattern) or pattern in url.split('/')[-1]:
                return False
        
        # Blog yazÄ±sÄ± gibi gÃ¶rÃ¼nÃ¼yor mu?
        return len(url.split('/')) >= 5 and '/blog/' in url
    
    def get_all_blog_links(self) -> List[str]:
        """
        TÃ¼m blog linklerini topla
        """
        try:
            self.setup_driver()
            
            # Ana blog sayfasÄ±
            blog_url = BASE_URL + "blog/"
            links_from_blog = self.scroll_and_load_more(blog_url)
            
            # Category sayfasÄ±
            category_url = BASE_URL + "category/blog/"
            links_from_category = self.scroll_and_load_more(category_url)
            
            # TÃ¼m linkleri birleÅŸtir
            all_links = links_from_blog.union(links_from_category)
            
            print(f"ğŸ¯ Toplam {len(all_links)} benzersiz blog yazÄ±sÄ± bulundu!")
            return list(all_links)
            
        except Exception as e:
            print(f"âŒ Selenium scraping hatasÄ±: {e}")
            return []
        
        finally:
            if self.driver:
                self.driver.quit()
                print("ğŸ”’ WebDriver kapatÄ±ldÄ±")
    
    def load_website_docs_advanced(self) -> List:
        """
        Selenium ile bulunan tÃ¼m linkleri WebBaseLoader ile yÃ¼kle
        """
        print("ğŸš€ GeliÅŸmiÅŸ scraping baÅŸlÄ±yor...")
        
        # TÃ¼m blog linklerini bul
        blog_links = self.get_all_blog_links()
        
        if not blog_links:
            print("âŒ Blog linki bulunamadÄ±!")
            return []
        
        # Ana sayfa + tÃ¼m blog yazÄ±larÄ±
        all_urls = [BASE_URL] + blog_links
        
        print(f"ğŸ“¥ {len(all_urls)} URL yÃ¼kleniyor...")
        
        try:
            loader = WebBaseLoader(all_urls)
            docs = loader.load()
            print(f"âœ… {len(docs)} dokÃ¼man baÅŸarÄ±yla yÃ¼klendi!")
            return docs
            
        except Exception as e:
            print(f"âŒ WebBaseLoader hatasÄ±: {e}")
            return []
    
    def save_advanced_data(self, docs, filename: str = None):
        """
        GeliÅŸmiÅŸ scraping verilerini kaydet
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"advanced_scraped_data_{timestamp}.json"
        
        # data/raw klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        raw_data_dir = os.path.join(os.path.dirname(__file__), "..", "data", "raw")
        os.makedirs(raw_data_dir, exist_ok=True)
        
        filepath = os.path.join(raw_data_dir, filename)
        
        # DokÃ¼manlarÄ± JSON formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
        data_to_save = []
        for doc in docs:
            data_to_save.append({
                "content": doc.page_content,
                "metadata": doc.metadata,
                "scraped_at": datetime.now().isoformat(),
                "scraper_type": "selenium_advanced"
            })
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ GeliÅŸmiÅŸ scraping verileri kaydedildi: {filepath}")
        return filepath

def advanced_scrape_and_save():
    """
    Ana geliÅŸmiÅŸ scraping fonksiyonu
    """
    scraper = AdvancedScraper(headless=True)
    docs = scraper.load_website_docs_advanced()
    
    if docs:
        filepath = scraper.save_advanced_data(docs)
        print(f"ğŸ‰ GeliÅŸmiÅŸ scraping tamamlandÄ±! {len(docs)} dokÃ¼man kaydedildi.")
        return docs
    else:
        print("âŒ GeliÅŸmiÅŸ scraping baÅŸarÄ±sÄ±z!")
        return []

def test_selenium_scraping():
    """
    Selenium scraper'Ä± test etmek iÃ§in Ã¶nizleme fonksiyonu
    """
    print("ğŸ” Selenium scraping testi baÅŸlÄ±yor...\n")
    
    try:
        scraper = AdvancedScraper(headless=True)
        
        # Sadece linkleri bul (dokÃ¼manlarÄ± yÃ¼kleme)
        print("ğŸ“„ Blog linklerini tespit ediliyor...")
        blog_links = scraper.get_all_blog_links()
        
        if not blog_links:
            print("âŒ HiÃ§ blog linki bulunamadÄ±!")
            return []
        
        print(f"âœ… Toplam {len(blog_links)} blog yazÄ±sÄ± bulundu!\n")
        
        # Ä°lk 10 linki gÃ¶ster
        print("ğŸ” Bulunan blog yazÄ±larÄ± (ilk 10):")
        for i, link in enumerate(blog_links[:10], 1):
            print(f"  {i}. {link}")
        
        if len(blog_links) > 10:
            print(f"  ... ve {len(blog_links) - 10} tane daha")
        
        # KullanÄ±cÄ±ya seÃ§enek sun
        print(f"\nğŸ“Š SonuÃ§: {len(blog_links)} blog yazÄ±sÄ± tespit edildi")
        print("ğŸ’¡ Bu sayÄ± beklediÄŸiniz 60'a yakÄ±n mÄ±?")
        
        return blog_links
        
    except Exception as e:
        print(f"âŒ Test sÄ±rasÄ±nda hata: {e}")
        return []

def test_with_sample_content():
    """
    BirkaÃ§ blog yazÄ±sÄ±nÄ±n iÃ§eriÄŸini de test et
    """
    print("ğŸ” Ä°Ã§erik testi baÅŸlÄ±yor...\n")
    
    try:
        scraper = AdvancedScraper(headless=True)
        
        # Ã–nce linkleri bul
        blog_links = scraper.get_all_blog_links()
        
        if not blog_links:
            print("âŒ Test iÃ§in blog linki bulunamadÄ±!")
            return
        
        # Ä°lk 3 blog yazÄ±sÄ±nÄ±n iÃ§eriÄŸini yÃ¼kle
        test_urls = [BASE_URL] + blog_links[:3]
        print(f"ğŸ“¥ Test iÃ§in {len(test_urls)} URL yÃ¼kleniyor...")
        
        from langchain_community.document_loaders import WebBaseLoader
        loader = WebBaseLoader(test_urls)
        docs = loader.load()
        
        print(f"âœ… {len(docs)} dokÃ¼man yÃ¼klendi\n")
        
        # Ä°Ã§erik Ã¶nizlemesi
        for i, doc in enumerate(docs):
            print(f"ğŸ“„ ({i+1}) Kaynak: {doc.metadata.get('source', 'Bilinmiyor')}")
            content_preview = doc.page_content[:200].replace('\n', ' ').strip()
            print(f"ğŸ“ Ä°Ã§erik: {content_preview}...")
            print("-" * 80)
        
        return docs
        
    except Exception as e:
        print(f"âŒ Ä°Ã§erik testi sÄ±rasÄ±nda hata: {e}")
        return []

if __name__ == "__main__":
    print("ğŸš€ Selenium tabanlÄ± geliÅŸmiÅŸ scraper baÅŸlatÄ±lÄ±yor...\n")
    print("SeÃ§enekler:")
    print("1ï¸âƒ£ Link testi - Sadece blog linklerini bul")
    print("2ï¸âƒ£ Ä°Ã§erik testi - BirkaÃ§ yazÄ±nÄ±n iÃ§eriÄŸini de test et") 
    print("3ï¸âƒ£ Tam scraping - TÃ¼m verileri Ã§ek ve kaydet\n")
    
    # VarsayÄ±lan olarak link testini Ã§alÄ±ÅŸtÄ±r
    print("ğŸ”„ Link testi Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...\n")
    links = test_selenium_scraping()
    
    if links and len(links) >= 10:  # En az 10 link bulunmuÅŸsa
        print(f"\nâœ¨ Test baÅŸarÄ±lÄ±! {len(links)} link bulundu.")
        print("ğŸ’¾ TÃ¼m verileri kaydetmek iÃ§in advanced_scrape_and_save() Ã§alÄ±ÅŸtÄ±rÄ±n.")
    else:
        print(f"\nâš ï¸  Sadece {len(links) if links else 0} link bulundu.")
        print("ğŸ”§ Scraping stratejisini gÃ¶zden geÃ§irmek gerekebilir.")
