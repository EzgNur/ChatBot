"""
FastAPI Web Servisi - GROQ Chatbot Entegrasyonu
Oktay √ñzdemir Blog Chatbot API
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, Dict, List
import os
from datetime import datetime
import uvicorn
import tempfile
import os
import glob
import math
from pydub import AudioSegment
import ffmpeg
FFMPEG_CMD = 'ffmpeg'
try:
    import imageio_ffmpeg  # ffmpeg ikilisi yoksa g√∂m√ºl√º olanƒ± kullanmak i√ßin
    FFMPEG_CMD = imageio_ffmpeg.get_ffmpeg_exe() or 'ffmpeg'
except Exception:
    # imageio-ffmpeg kurulu deƒüilse sistemdeki ffmpeg kullanƒ±lacaktƒ±r
    FFMPEG_CMD = 'ffmpeg'

# Chatbot import
from backend.core.chatbot.bot import FreeChatBot
from vectorstore.build_store import OptimizedVectorStoreBuilder
from groq import Groq
import language_tool_python
from text_normalizer import normalize_text_pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter

# FastAPI app
app = FastAPI(
    title="Oktay √ñzdemir Blog Chatbot API",
    description="GROQ destekli hukuk ve g√∂√ß uzmanƒ± chatbot",
    version="1.0.0"
)

# CORS middleware (web aray√ºz√º i√ßin)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # React/Vue geli≈ütirme
        "http://localhost:8080",  # Vite/Nuxt
        "http://127.0.0.1:5500",  # Live Server
        "file://",                # Yerel HTML dosyalarƒ±
        "*"                       # Geli≈ütirme i√ßin (production'da kaldƒ±rƒ±n)
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

# Global chatbot instance
chatbot = None
builder = OptimizedVectorStoreBuilder()
groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
try:
    tool_tr = language_tool_python.LanguageTool('tr-TR')
except Exception:
    # Java yoksa public API moduna d√º≈ü
    try:
        tool_tr = language_tool_python.LanguageToolPublicAPI('tr-TR')
    except Exception:
        tool_tr = None

# Basit transkript temizleyici (heuristic)
def clean_transcript_text(text: str) -> str:
    if not text:
        return ""
    import re
    t = text
    # Yaygƒ±n giri≈ü/√ßƒ±kƒ±≈ü ve CTA ifadeleri
    patterns = [
        r"^\s*(merhaba(lar)?|selam(lar)?).*$",
        r"abone ol(mayƒ±)?|beƒüen(meyi)?|kanal(ƒ±|ƒ±mƒ±za)|takip etmeyi",
        r"(like|subscribe|yorumlarƒ±nƒ±zƒ±)",
        r"flash\s*flash",
        r"(g√∂r√º≈ü√ºr√ºz|te≈üekk√ºrler|izlediƒüiniz i√ßin te≈üekk√ºrler).*",
        r"sanal deƒüil ger√ßek ofis",
        r"(√ßayƒ±nƒ±|kahveni) i√ßersin",
        r"altyazƒ±.*$",
    ]
    for p in patterns:
        t = re.sub(p, "", t, flags=re.IGNORECASE | re.MULTILINE)

    # Sƒ±k yazƒ±m d√ºzeltmeleri (basit e≈üle≈ütirme)
    replacements = {
        r"\b√∂n\s*olay\b": "√∂n onay",
        r"\b√∂n\s*anay\b": "√∂n onay",
        r"Agent√ºr\s*F[√ºu]hrer\s*Arb[ae]yt": "Agentur f√ºr Arbeit",
        r"Agent√ºr\s*F[√ºu]hrer\s*Arbeit": "Agentur f√ºr Arbeit",
        r"Agentur\s*f[√ºu]r\s*Arbayt": "Agentur f√ºr Arbeit",
        r"\b[ƒ∞I]data\b": "ƒ∞data",
        # 'ƒ∞data' yaygƒ±n hatalarƒ±
        r"\biy[ƒ±i]\s*data\b": "ƒ∞data",
        r"\biy[ƒ±i]\s*dataya\b": "ƒ∞data'ya",
        r"\biy[ƒ±i]\s*dataya\b": "ƒ∞data'ya",
        r"\biy[ƒ±i]\s*datadan\b": "ƒ∞data'dan",
        r"\biy[ƒ±i]\s*datada\b": "ƒ∞data'da",
        r"\biy[ƒ±i]\s*datayƒ±\b": "ƒ∞data'yƒ±",
    }
    for pat, rep in replacements.items():
        t = re.sub(pat, rep, t, flags=re.IGNORECASE)

    # Zaman damgalarƒ± ve gereksiz tekrar bo≈üluklar
    t = re.sub(r"\b\d{1,2}:\d{2}(:\d{2})?\b", " ", t)
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"\s*\n\s*", "\n", t)
    t = t.strip()
    
    # LanguageTool ile ek yazƒ±m/dil kontrol√º
    if tool_tr is not None:
        try:
            matches = tool_tr.check(t)
            t = language_tool_python.utils.correct(t, matches)
        except Exception:
            pass
    return t

# Cevap parlatma: √ñzet ‚Üí Detaylar ‚Üí Kaynaklar + footer ayrƒ± blok
def polish_answer(answer_text: str, source_links: List[Dict]) -> str:
    import re
    text = answer_text or ""
    # Varsa √∂nceki footer/ayƒ±rƒ±cƒ±yƒ± kaldƒ±r
    text = re.sub(r"\n+‚Äî+\n+.*$", "", text, flags=re.DOTALL)
    text = re.sub(r"\n+üìö.*$", "", text, flags=re.DOTALL)

    # C√ºmlelere ayƒ±r ve √∂zet √ßƒ±kar
    sentences = re.split(r"(?<=[\.!?])\s+", text.strip())
    summary = " ".join(sentences[:2]).strip()
    rest = " ".join(sentences[2:]).strip()

    details = []
    if rest:
        # Noktalƒ± c√ºmleleri maddeye √ßevir (√ßok uzun maddeleri kƒ±salt)
        chunks = re.split(r"(?<=[\.!?])\s+", rest)
        for ch in chunks:
            c = ch.strip()
            # ƒ∞√ßeride tekrar kaynak/footeri andƒ±ran c√ºmleleri at
            if (
                len(c) > 0 and
                not c.lower().startswith("kaynak") and
                not c.startswith("‚Äî") and
                not c.startswith("---") and
                not c.startswith("üìö") and
                "B√ºt√ºn bilgiler" not in c
            ):
                # Bozuk madde ba≈ülarƒ±nƒ± normalize et
                c = re.sub(r"^([\-*‚Ä¢]+\s*)+", "", c)
                details.append(c)

    # Detaylarƒ± b√∂l√ºmlere ayƒ±r: ≈ûartlar / ƒ∞stisnalar / Adƒ±mlar
    temel: List[str] = []
    istisna: List[str] = []
    adim: List[str] = []

    def looks_like_step(s: str) -> bool:
        return bool(re.match(r"^(adƒ±m|√∂nce|ardƒ±ndan|sonra|1\.|2\.|3\.|[0-9]+\))", s.strip(), flags=re.IGNORECASE))

    for d in details:
        dl = d.lower()
        if any(k in dl for k in ["istisna", "hari√ß", "deƒüilse", "olmadƒ±ƒüƒ±", "olmazsa"]):
            istisna.append(d)
        elif looks_like_step(dl):
            # Numerik adƒ±mlarƒ± normalize et
            d_clean = re.sub(r"^(\d+\.|\d+\))\s*", "", d).strip()
            adim.append(d_clean)
        else:
            temel.append(d)

    parts: List[str] = []
    if summary:
        # Ba≈üta "√ñzet:" etiketi olmadan doƒüal bir ilk paragraf olarak kalsƒ±n
        parts.append(summary)
    if temel:
        parts.append("\n≈ûartlar:\n" + "\n".join([f"- {d}" for d in temel]))
    if istisna:
        parts.append("\nƒ∞stisnalar:\n" + "\n".join([f"- {d}" for d in istisna]))
    if adim:
        parts.append("\nAdƒ±mlar:\n" + "\n".join([f"- {d}" for d in adim]))

    # Footer'ƒ± ayrƒ± blok olarak ekle
    footer = "\n\n‚Äî\n\nüìö B√ºt√ºn bilgiler Oktay √ñzdemir Danƒ±≈ümanlƒ±k web sitemizden alƒ±nmƒ±≈ütƒ±r. Daha detaylƒ± bilgi almak i√ßin [Oktay √ñzdemir Danƒ±≈ümanlƒ±k](https://oktayozdemir.com.tr) web sitemizi ziyaret edebilirsiniz."

    # Kaynak linklerini istemci tarafƒ±nda g√∂steriyoruz; mesaj i√ßinde g√∂r√ºnmesin
    return "\n\n".join([p for p in parts if p.strip()]) + footer

# Bilgilendirici √∂nizleme (filler c√ºmleleri √ßƒ±kar, √ßekirdek bilgiyi √∂ne al)
def generate_informative_preview(text: str, max_chars: int = 1400) -> str:
    import re
    if not text:
        return ""
    sentences = re.split(r"(?<=[\.!?])\s+", text.strip())

    filler_patterns = [
        r"bildirimleri a√ßƒ±n|takip edin|abone ol|beƒüenin|payla≈üƒ±n",
        r"sanal deƒüil ger√ßek ofis",
        r"dostlarƒ±nƒ±za tavsiye|arkada≈ü|e≈üinizi|dostunuzu",
        r"videonun|kanalƒ±n|yorumlarda|a√ßƒ±klama b√∂l√ºm√ºnde",
        r"giri≈ü|kapanƒ±≈ü|merhaba|selam",
    ]
    def is_filler(s: str) -> bool:
        s_low = s.lower()
        return any(re.search(p, s_low) for p in filler_patterns) or len(s_low) < 25

    # Bilgi sinyali: rakam, yasa/kod, vize kodlarƒ±, anahtar kelimeler
    signal_patterns = [
        r"\b(18a|18b|18g|19c|81a|anabin|zab|ezb|mavi kart|blue card)\b",
        r"\b\d{4}\b|%|‚Ç¨|euro|g√ºn|hafta|ay|yƒ±l",
        r"vize|ba≈üvuru|randevu|ikamet|√ßalƒ±≈üma|s√∂zle≈üme|≈üart|gerekli|belge|√ºcret|denklik",
    ]
    def score_sentence(s: str) -> int:
        score = 0
        sl = s.lower()
        for p in signal_patterns:
            if re.search(p, sl):
                score += 1
        if len(s) > 80:
            score += 1
        return score

    informative = [s for s in sentences if not is_filler(s)]
    informative_sorted = sorted(informative, key=score_sentence, reverse=True)

    out: List[str] = []
    total = 0
    for s in informative_sorted:
        if total + len(s) + 1 > max_chars:
            break
        out.append(s)
        total += len(s) + 1
    return " ".join(out).strip()

# Request/Response modelleri
class ChatRequest(BaseModel):
    question: str
    model: Optional[str] = "groq"  # "groq" veya "openai"
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    answer: str
    sources: List[Dict]
    source_links: List[Dict]
    response_time: str
    chunks_used: int
    model: str
    timestamp: str
    action_buttons: Optional[List[Dict]] = None  # Yeni eklendi
    special_response: Optional[bool] = False  # Yeni eklendi
    special_type: Optional[str] = None  # consultant | eligibility | category_menu | None
    categories: Optional[List[Dict]] = None  # Kategori men√ºs√º i√ßin

class HealthResponse(BaseModel):
    status: str
    message: str
    timestamp: str
    vectorstore_loaded: bool
    api_keys_configured: Dict[str, bool]

@app.on_event("startup")
async def startup_event():
    """Uygulama ba≈ülatƒ±lƒ±rken chatbot'u initialize et"""
    global chatbot
    try:
        print("üöÄ FastAPI Chatbot ba≈ülatƒ±lƒ±yor...")
        
        # GROQ chatbot'u ba≈ülat
        chatbot = FreeChatBot()
        
        if chatbot.initialize():
            print("‚úÖ GROQ Chatbot ba≈üarƒ±yla y√ºklendi!")
            
            # Eƒüitilmi≈ü modeli y√ºkle
            model_path = "./trained_rag_lora_model"
            if os.path.exists(model_path):
                if chatbot.load_trained_model(model_path):
                    print("‚úÖ Eƒüitilmi≈ü model ba≈üarƒ±yla y√ºklendi!")
                    print("üöÄ Hibrit sistem aktif: RAG + Eƒüitilmi≈ü Model")
                else:
                    print("‚ö†Ô∏è Eƒüitilmi≈ü model y√ºklenemedi, sadece RAG kullanƒ±lacak")
            else:
                print("‚ö†Ô∏è Eƒüitilmi≈ü model bulunamadƒ±, sadece RAG kullanƒ±lacak")
        else:
            print("‚ùå GROQ Chatbot y√ºklenemedi!")
            
    except Exception as e:
        print(f"‚ùå Startup hatasƒ±: {e}")

@app.get("/", response_model=Dict[str, str])
async def root():
    """Ana sayfa"""
    return {
        "message": "Oktay √ñzdemir Blog Chatbot API",
        "version": "1.0.0",
        "status": "active",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Sistem durumu kontrol√º"""
    vectorstore_loaded = chatbot is not None and chatbot.vectorstore is not None
    
    api_keys = {
        "groq": bool(os.getenv("GROQ_API_KEY")),
        "openai": bool(os.getenv("OPENAI_API_KEY"))
    }
    
    resp = HealthResponse(
        status="healthy" if vectorstore_loaded else "degraded",
        message="Chatbot hazƒ±r" if vectorstore_loaded else "Vector store y√ºklenemedi",
        timestamp=datetime.now().isoformat(),
        vectorstore_loaded=vectorstore_loaded,
        api_keys_configured=api_keys
    )
    try:
        print(f"ü©∫ /health -> status={resp.status} vectorstore_loaded={resp.vectorstore_loaded} api_keys={resp.api_keys_configured}")
    except Exception:
        pass
    return resp

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    """Chatbot'a soru sor"""
    try:
        print(f"üßæ Gelen soru: {request.question}")
    except Exception:
        pass
    if not chatbot:
        raise HTTPException(status_code=503, detail="Chatbot hen√ºz y√ºklenmedi")
    
    if not request.question.strip():
        raise HTTPException(status_code=400, detail="Soru bo≈ü olamaz")
    
    try:
        # Hafƒ±za: kƒ±sa ge√ßmi≈üi al (varsa)
        session_id = request.session_id or "__default__"
        if not hasattr(app.state, "memory"):
            app.state.memory = {}
        memory: Dict[str, List[Dict[str, str]]] = app.state.memory
        turns = memory.get(session_id, [])

        # Niyet: detay
        qlower = request.question.lower()
        detail_intent = any(p in qlower for p in ["detay", "ayrƒ±ntƒ±", "daha fazla bilgi", "detaylandƒ±r", "uzat"])

        expanded_question = request.question
        if detail_intent and turns:
            # Son kullanƒ±cƒ± sorusu ve asistan cevabƒ±nƒ± √∂zellikle vurgula
            last_user = next((t.get("content", "") for t in reversed(turns) if t.get("role") == "user"), "")
            last_assistant = next((t.get("content", "") for t in reversed(turns) if t.get("role") == "assistant"), "")

            # Son 3 turu kƒ±sa bir izlek olarak ekle (1200 karakter sƒ±nƒ±rƒ±)
            history_text = []
            for t in turns[-6:]:
                role = t.get("role")
                content = t.get("content", "")
                prefix = "Kullanƒ±cƒ±" if role == "user" else "Asistan"
                history_text.append(f"{prefix}: {content}")
            history_join = " \n".join(history_text)[-1200:]

            expanded_question = (
                "Bu bir takip isteƒüidir. A≈üaƒüƒ±daki √∂nceki sorunun ayni KONUSUNU daha detaylƒ±, derin ve d√ºzenli olarak geni≈ület. Konu dƒ±≈üƒ±na √ßƒ±kma.\n" \
                "√ñnceki kullanƒ±cƒ± sorusu: " + last_user + "\n" \
                "√ñnceki asistan cevabƒ± (√∂zetlenecek/geli≈ütirilecek): " + last_assistant + "\n\n" \
                "Kƒ±sa ge√ßmi≈ü: " + history_join + "\n\n" \
                "Yeni talep: " + request.question
            )

        # Kategori se√ßimi kontrol√º
        selected_category = None
        if "kategori" in request.question.lower() or "ba≈ülƒ±k" in request.question.lower() or "hangi" in request.question.lower():
            # Kategori men√ºs√º d√∂nd√ºr
            result = chatbot.get_category_menu()
        else:
            # Kategori tespiti
            category_indicators = {
                "hukuk_goc": ["hukuk", "g√∂√ß", "vize", "ikamet", "iltica", "mavi kart", "81a"],
                "mesleki_egitim": ["meslek", "eƒüitim", "denklik", "kalfalƒ±k", "ustalƒ±k", "√∂n lisans"],
                "is_calisma": ["i≈ü", "√ßalƒ±≈üma", "≈üof√∂r", "usta", "kasap", "a≈ü√ßƒ±", "elektrik√ßi"],
                "yerlesim_yasam": ["yerle≈üim", "ya≈üam", "anmeldung", "dil", "a2", "b1"],
                "mali_konular": ["maa≈ü", "har√ß", "mali", "euro", "√ºcret", "masraf"],
                "ulke_bazli": ["almanya", "ingiltere", "√ºlke", "scale-up"],
                "surec_prosedur": ["s√ºre√ß", "prosed√ºr", "ba≈üvuru", "evrak", "s√ºre"],
                "ozel_durumlar": ["ya≈ü", "√∂zel", "durum", "fakt√∂r", "seviye"]
            }
            
            for category_id, indicators in category_indicators.items():
                if any(indicator in request.question.lower() for indicator in indicators):
                    selected_category = category_id
                    break
            
            # GROQ ile cevap √ºret (kategori odaklƒ±)
            result = chatbot.ask_groq(expanded_question, selected_category=selected_category)

        # Kategori men√ºs√º kontrol√º
        if result.get("special_response") and result.get("type") == "category_menu":
            # Kategori men√ºs√º i√ßin √∂zel response
            resp = ChatResponse(
                answer=result["answer"],
                sources=result.get("sources", []),
                source_links=result.get("source_links", []),
                response_time="0s",
                chunks_used=0,
                model="category_menu",
                timestamp=datetime.now().isoformat(),
                action_buttons=result.get("action_buttons", []),
                special_response=True,
                special_type="category_menu"
            )
            # Kategori bilgilerini ekle
            resp.categories = result.get("categories", [])
        else:
            # Normal cevap i≈üleme
            try:
                cfg = os.path.join(os.path.dirname(__file__), 'config', 'text_rules.yaml')
                stage1 = normalize_text_pipeline(result.get("answer", ""), cfg)
                cleaned_answer = clean_transcript_text(stage1)
                cleaned_answer = polish_answer(cleaned_answer, result.get("source_links", []))
            except Exception:
                cleaned_answer = result.get("answer", "")

            resp = ChatResponse(
                answer=cleaned_answer,
                sources=result.get("sources", []),
                source_links=result.get("source_links", []),
                response_time=result["response_time"],
                chunks_used=result["chunks_used"],
                model=result.get("model", "llama-3.3-70b-versatile"),
                timestamp=result["timestamp"],
                action_buttons=result.get("action_buttons"),
                special_response=result.get("special_response", False),
                special_type=result.get("special_type")
            )
        try:
            ans_preview = (resp.answer or "")[:180].replace("\n", " ") + ("..." if len(resp.answer or "") > 180 else "")
            print(
                f"üí¨ /ask -> time={resp.response_time} chunks={resp.chunks_used} model={resp.model} "
                f"sources={len(resp.sources)} preview=\"{ans_preview}\""
            )
        except Exception:
            pass
        return resp
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chatbot hatasƒ±: {str(e)}")
    finally:
        # Hafƒ±zaya kaydet (son 3-5 turu tut)
        try:
            if 'result' in locals() and 'session_id' in locals() and hasattr(app.state, "memory"):
                memory = app.state.memory
                if session_id not in memory:
                    memory[session_id] = []
                memory[session_id].append({"role": "user", "content": request.question})
                # Temizlenmi≈ü cevabƒ± saklƒ±yoruz; gerekirse orijinal cevabƒ± da ekleyebilirsiniz
                memory[session_id].append({"role": "assistant", "content": cleaned_answer})
                # Son 6 kaydƒ± tut (3 tur)
                memory[session_id] = memory[session_id][-6:]
        except Exception:
            pass

@app.post("/ingest/video")
async def ingest_video(
    file: UploadFile = File(...),
    language: str = Form("tr"),
    title: str = Form("Video Transcript"),
    url: str = Form(""),
    author: str = Form("Video"),
    clean: bool = Form(True),
    dry_run: bool = Form(False)
):
    """Video y√ºkle ‚Üí ses ayƒ±kla ‚Üí Whisper ile transcribe ‚Üí FAISS'e ekle."""
    # 1) Temp kaydet
    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp_vid:
        tmp_vid.write(await file.read())
        video_path = tmp_vid.name

    audio_path = video_path + ".wav"
    try:
        # 2) Ses ayƒ±kla (16k mono WAV)
        stream = (
            ffmpeg
            .input(video_path)
            .output(audio_path, ac=1, ar=16000, format='wav', loglevel="error")
            .overwrite_output()
        )
        ffmpeg.run(stream, cmd=FFMPEG_CMD)

        # 3) Ses uzunluƒüunu √∂l√ß (ffprobe yerine pydub kullan)
        try:
            duration_sec = float(AudioSegment.from_wav(audio_path).duration_seconds)
        except Exception:
            duration_sec = 0.0

        # 413 hatasƒ±nƒ± √∂nlemek i√ßin ~9 dk'lƒ±k segmentlere b√∂l (540 sn)
        segment_paths = []
        if duration_sec > 540:
            seg_dir = tempfile.mkdtemp(prefix="segments_")
            seg_pattern = os.path.join(seg_dir, "part_%03d.wav")
            seg_stream = (
                ffmpeg
                .input(audio_path)
                .output(
                    seg_pattern,
                    f='segment',
                    segment_time=540,
                    ac=1,
                    ar=16000,
                    loglevel="error"
                )
                .overwrite_output()
            )
            ffmpeg.run(seg_stream, cmd=FFMPEG_CMD)
            segment_paths = sorted(glob.glob(os.path.join(seg_dir, "part_*.wav")))
        else:
            segment_paths = [audio_path]

        # 4) Her segmenti ayrƒ± ayrƒ± transcribe et ve birle≈ütir
        texts = []
        total_duration = 0.0
        for seg in segment_paths:
            with open(seg, "rb") as af:
                tr = groq_client.audio.transcriptions.create(
                    model="whisper-large-v3-turbo",
                    file=af,
                    language=language,
                    response_format="verbose_json"
                )
            texts.append(tr.text.strip())
            try:
                total_duration += float(getattr(tr, "duration", 0) or 0)
            except Exception:
                pass

        text = "\n\n".join(t for t in texts if t)
        duration = total_duration or duration_sec

        # 4) Transkript dosyasƒ±nƒ± kaydet (g√∂r√ºnt√ºlemek i√ßin)
        transcripts_dir = os.path.join(os.path.dirname(__file__), "data", "raw", "transcripts")
        os.makedirs(transcripts_dir, exist_ok=True)
        ts_name = datetime.now().strftime("%Y%m%d_%H%M%S")
        transcript_path = os.path.join(transcripts_dir, f"transcript_{ts_name}.txt")
        with open(transcript_path, "w", encoding="utf-8") as f:
            f.write(text)

        # 4.1) Temizleme (opsiyonel) - YAML pipeline + dil denetimi
        if clean:
            cfg = os.path.join(os.path.dirname(__file__), 'config', 'text_rules.yaml')
            stage1 = normalize_text_pipeline(text, cfg)
            final_text = clean_transcript_text(stage1)
        else:
            final_text = text
        cleaned_path = os.path.join(transcripts_dir, f"transcript_cleaned_{ts_name}.txt")
        with open(cleaned_path, "w", encoding="utf-8") as f:
            f.write(final_text)

        if dry_run:
            # Sadece √∂nizleme d√∂nd√ºr; FAISS'e ekleme
            # Ayrƒ±ca chunk tahmini ve bilgilendirici √∂nizleme ver
            splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
            chunks = splitter.split_text(final_text)
            informative_preview = generate_informative_preview(final_text, max_chars=1400)
            return {
                "ok": True,
                "dry_run": True,
                "chars": len(final_text),
                "title": title,
                "url": url,
                "transcript_file": os.path.relpath(transcript_path, os.path.dirname(__file__)),
                "cleaned_file": os.path.relpath(cleaned_path, os.path.dirname(__file__)),
                "preview": final_text[:1200],
                "informative_preview": informative_preview,
                "total_chunks_estimate": len(chunks),
                "first_chunks": chunks[:3]
            }
        else:
            # 5) FAISS'e ekle (temiz metin)
            meta = {"title": title, "url": url, "author": author, "source_type": "video_transcript", "duration": duration}
            chunks_added = builder.add_transcript_to_vectorstore(final_text, meta)

            # 6) API'nin anlƒ±k istatistiklerinde de g√∂r√ºns√ºn diye chatbot'un vectorstore'unu yeniden y√ºkle
            global chatbot
            try:
                if chatbot is not None:
                    vs_path = os.path.join(os.path.dirname(__file__), "data", "vectorstore")
                    chatbot.vectorstore = builder.load_vectorstore(vs_path)
            except Exception:
                pass

            return {
                "ok": True,
                "dry_run": False,
                "chars": len(final_text),
                "chunks_added": chunks_added,
                "title": title,
                "url": url,
                "transcript_file": os.path.relpath(transcript_path, os.path.dirname(__file__)),
                "cleaned_file": os.path.relpath(cleaned_path, os.path.dirname(__file__)),
                "preview": final_text[:500]
            }
    finally:
        for p in [video_path, audio_path]:
            try:
                os.remove(p)
            except Exception:
                pass

@app.post("/ingest/transcript")
async def ingest_transcript(
    text: str = Form(...),
    title: str = Form("Video Transcript"),
    url: str = Form(""),
    author: str = Form("Video"),
    clean: bool = Form(True)
):
    """Hazƒ±r metin transkripti (veya d√ºzenlenmi≈ü metni) vekt√∂r store'a ekler."""
    original = text
    if clean:
        cfg = os.path.join(os.path.dirname(__file__), 'config', 'text_rules.yaml')
        stage1 = normalize_text_pipeline(text, cfg)
        cleaned = clean_transcript_text(stage1)
    else:
        cleaned = text

    transcripts_dir = os.path.join(os.path.dirname(__file__), "data", "raw", "transcripts")
    os.makedirs(transcripts_dir, exist_ok=True)
    ts_name = datetime.now().strftime("%Y%m%d_%H%M%S")
    raw_path = os.path.join(transcripts_dir, f"manual_raw_{ts_name}.txt")
    with open(raw_path, "w", encoding="utf-8") as f:
        f.write(original)

    cleaned_path = os.path.join(transcripts_dir, f"manual_cleaned_{ts_name}.txt")
    with open(cleaned_path, "w", encoding="utf-8") as f:
        f.write(cleaned)

    meta = {"title": title, "url": url, "author": author, "source_type": "video_transcript"}
    chunks_added = builder.add_transcript_to_vectorstore(cleaned, meta)

    # Vectorstore'u yeniden y√ºkle ki /stats anƒ±nda g√ºncellensin
    global chatbot
    try:
        if chatbot is not None:
            vs_path = os.path.join(os.path.dirname(__file__), "data", "vectorstore")
            chatbot.vectorstore = builder.load_vectorstore(vs_path)
    except Exception:
        pass

    return {
        "ok": True,
        "chars_raw": len(original),
        "chars_cleaned": len(cleaned),
        "chunks_added": chunks_added,
        "raw_file": os.path.relpath(raw_path, os.path.dirname(__file__)),
        "cleaned_file": os.path.relpath(cleaned_path, os.path.dirname(__file__)),
        "clean_preview": cleaned[:500]
    }

@app.get("/models")
async def get_available_models():
    """Mevcut model listesi"""
    return {
        "groq": {
            "llama-3.3-70b-versatile": "Ana model (√∂nerilen)",
            "llama-3.1-8b-instant": "Hƒ±zlƒ± model",
            "gemma2-9b-it": "Google modeli"
        },
        "openai": {
            "gpt-4o-mini": "Maliyet etkin",
            "gpt-4o": "En iyi kalite"
        }
    }

@app.get("/stats")
async def get_stats():
    """Sistem istatistikleri"""
    if not chatbot or not chatbot.vectorstore:
        return {"error": "Vector store y√ºklenmemi≈ü"}
    
    doc_count = chatbot.vectorstore.index.ntotal
    embedding_model = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    return {
        "vectorstore": {
            "total_chunks": doc_count,
            "embedding_model": embedding_model,
            "status": "loaded"
        },
        "api_status": {
            "groq": bool(os.getenv("GROQ_API_KEY")),
            "openai": bool(os.getenv("OPENAI_API_KEY"))
        },
        "uptime": datetime.now().isoformat()
    }

if __name__ == "__main__":
    print("üåê FastAPI Chatbot ba≈ülatƒ±lƒ±yor...")
    print("API Docs: http://localhost:8000/docs")
    print("Health Check: http://localhost:8000/health")
    print("Chat Endpoint: POST http://localhost:8000/ask")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )